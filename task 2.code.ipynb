{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def crawl_and_scrape(urls):\n",
        "\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "    content = []\n",
        "    for url in urls:\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            text = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
        "            content.append(text)\n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping {url}: {e}\")\n",
        "    return ' '.join(content)\n",
        "\n",
        "\n",
        "def chunk_text(text, chunk_size=100):\n",
        "\n",
        "    words = text.split()\n",
        "    return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "\n",
        "def store_embeddings(chunks, model):\n",
        "\n",
        "    embeddings = model.encode(chunks)\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(embeddings)\n",
        "    return index, chunks\n",
        "\n",
        "def query_vector_database(query, index, model, chunks, top_k=3):\n",
        "\n",
        "    query_embedding = model.encode([query])\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "    return [chunks[i] for i in indices[0]]\n",
        "\n",
        "def generate_response(results):\n",
        "\n",
        "    return \"\\n\".join(results)\n",
        "\n",
        "def main():\n",
        "    # Step 1: Define URLs\n",
        "    urls = [\n",
        "        \"https://www.uchicago.edu/\",\n",
        "        \"https://www.washington.edu/\",\n",
        "        \"https://www.stanford.edu/\",\n",
        "        \"https://und.edu/\",\n",
        "    ]\n",
        "\n",
        "    # Step 2: Initialize embedding model\n",
        "    print(\"Loading embedding model...\")\n",
        "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Pre-trained model\n",
        "\n",
        "    # Step 3: Crawl and scrape websites\n",
        "    print(\"Crawling and scraping websites...\")\n",
        "    website_data = crawl_and_scrape(urls)\n",
        "\n",
        "    # Step 4: Chunk and embed content\n",
        "    print(\"Chunking and embedding content...\")\n",
        "    chunks = chunk_text(website_data)\n",
        "    index, stored_chunks = store_embeddings(chunks, model)\n",
        "\n",
        "    # Step 5: Handle user query\n",
        "    query = input(\"Enter your query: \")\n",
        "    print(\"Searching content...\")\n",
        "    answers = query_vector_database(query, index, model, stored_chunks)\n",
        "    print(\"Generating response...\")\n",
        "    response = generate_response(answers)\n",
        "    print(response)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_mvfOalrPLU",
        "outputId": "5718d37b-776c-49ae-8e4f-3f3f478f06be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.9.0.post1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Loading embedding model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawling and scraping websites...\n",
            "Chunking and embedding content...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GGRMR2d0sBOW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}